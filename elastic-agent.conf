# logstash.conf - Daily GB limits with tiered responses

input {
  # Your existing inputs here
  beats {
    host => "${LOGSTASH_LISTEN_IP:0.0.0.0}"
    port => "${LOGSTASH_LISTEN_PORT:5044}"
  }
}

filter {
  # Add organization identifier to all events
  mutate {
    add_field => {
      "organization.id" => "${ORGANIZATION_ID}"
    }
  }

  # Initialize tracking variables
  ruby {
    init => "
      require 'json'
      require 'fileutils'
      @daily_limit_gb = (ENV['LOGSTASH_DAILY_LIMIT_GB'] || '10').to_f
      @tracking_file = ENV['LOGSTASH_TRACKING_FILE'] || '/opt/logstash/data/daily_usage.json'
      @current_date = Time.now.strftime('%Y-%m-%d')
    "
    code => "
      # Load or initialize daily tracking
      if File.exist?(@tracking_file)
        usage_data = JSON.parse(File.read(@tracking_file)) rescue {}
      else
        usage_data = {}
      end
      
      # Check if new day and create reset alert
      if usage_data['date'] != @current_date
        # If we had previous data, create reset alert
        if usage_data['date'] && usage_data['bytes_sent'] && usage_data['bytes_sent'] > 0
          event.set('[@metadata][create_reset_alert]', true)
          event.set('[@metadata][previous_date]', usage_data['date'])
          event.set('[@metadata][previous_gb_sent]', usage_data['bytes_sent'] / (1024.0 ** 3))
        end
        
        usage_data = {
          'date' => @current_date,
          'bytes_sent' => 0,
          'threshold_100_hit' => false,
          'threshold_125_hit' => false,
          'gb_alerts_sent' => []  # Track which GB thresholds have been alerted
        }
      end
      
      # Calculate event size (approximate)
      event_size = event.to_json.bytesize
      usage_data['bytes_sent'] += event_size
      
      # Convert to GB for threshold checking
      gb_sent = usage_data['bytes_sent'] / (1024.0 ** 3)
      usage_percent = (gb_sent / @daily_limit_gb) * 100
      gb_over_limit = gb_sent - @daily_limit_gb
      
      # Set event fields for routing decisions
      event.set('[@metadata][daily_gb_sent]', gb_sent)
      event.set('[@metadata][usage_percent]', usage_percent)
      event.set('[@metadata][daily_limit_gb]', @daily_limit_gb)
      event.set('[@metadata][gb_over_limit]', gb_over_limit)
      
      # Check main thresholds
      if usage_percent >= 100 && !usage_data['threshold_100_hit']
        usage_data['threshold_100_hit'] = true
        event.set('[@metadata][create_100_alert]', true)
      end
      
      if usage_percent >= 125 && !usage_data['threshold_125_hit']
        usage_data['threshold_125_hit'] = true
        event.set('[@metadata][create_125_alert]', true)
      end
      
      # Check for 1GB increment alerts over limit
      if gb_over_limit > 0
        current_gb_threshold = gb_over_limit.floor + 1  # 1GB, 2GB, 3GB, etc.
        if !usage_data['gb_alerts_sent'].include?(current_gb_threshold)
          usage_data['gb_alerts_sent'] << current_gb_threshold
          event.set('[@metadata][create_gb_increment_alert]', true)
          event.set('[@metadata][gb_increment_threshold]', current_gb_threshold)
        end
      end
      
      # Save updated usage data
      File.write(@tracking_file, usage_data.to_json)
    "
  }
  
  # Create ECS-compliant alert events
  if [@metadata][create_100_alert] {
    clone {
      clones => ["threshold_alert"]
    }
  }
  
  if [@metadata][create_125_alert] {
    clone {
      clones => ["threshold_alert"]
    }
  }
  
  if [@metadata][create_gb_increment_alert] {
    clone {
      clones => ["threshold_alert"]
    }
  }
  
  if [@metadata][create_reset_alert] {
    clone {
      clones => ["reset_alert"]
    }
  }
  
  if [type] == "threshold_alert" {
    mutate {
      replace => {
        "@timestamp" => "%{+YYYY-MM-ddTHH:mm:ss.SSSZ}"
        "message" => "Daily Logstash ingestion threshold reached"
        "service.name" => "logstash"
        "service.type" => "ingestion"
        "log.level" => "WARN"
        "event.kind" => "alert"
        "event.category" => ["monitoring"]
        "event.type" => ["info"]
        "event.dataset" => "logstash.threshold"
      }
      add_field => {
        "logstash.daily_limit_gb" => "%{[@metadata][daily_limit_gb]}"
        "logstash.daily_gb_sent" => "%{[@metadata][daily_gb_sent]}"
        "logstash.usage_percent" => "%{[@metadata][usage_percent]}"
      }
    }
    
    if [@metadata][create_100_alert] {
      mutate {
        add_field => {
          "logstash.threshold_reached" => "100_percent"
          "logstash.action" => "dropping_metrics"
        }
      }
    }
    
    if [@metadata][create_125_alert] {
      mutate {
        add_field => {
          "logstash.threshold_reached" => "125_percent"
          "logstash.action" => "local_storage_enabled"
        }
      }
    }
    
    if [@metadata][create_gb_increment_alert] {
      mutate {
        replace => {
          "message" => "Daily Logstash ingestion exceeded limit by %{[@metadata][gb_increment_threshold]}GB"
          "log.level" => "ERROR"
        }
        add_field => {
          "logstash.threshold_reached" => "gb_increment"
          "logstash.action" => "local_storage_active"
          "logstash.gb_over_limit" => "%{[@metadata][gb_over_limit]}"
          "logstash.gb_increment_threshold" => "%{[@metadata][gb_increment_threshold]}"
        }
      }
    }
  }
  
  if [type] == "reset_alert" {
    mutate {
      replace => {
        "@timestamp" => "%{+YYYY-MM-ddTHH:mm:ss.SSSZ}"
        "message" => "Daily Logstash ingestion counter reset - normal operations resumed"
        "service.name" => "logstash"
        "service.type" => "ingestion"
        "log.level" => "INFO"
        "event.kind" => "alert"
        "event.category" => ["monitoring"]
        "event.type" => ["change"]
        "event.dataset" => "logstash.reset"
      }
      add_field => {
        "logstash.daily_limit_gb" => "%{[@metadata][daily_limit_gb]}"
        "logstash.action" => "normal_operations_resumed"
        "logstash.previous_date" => "%{[@metadata][previous_date]}"
        "logstash.previous_gb_sent" => "%{[@metadata][previous_gb_sent]}"
      }
    }
  }
}

output {
  # Send threshold alerts to Elastic Cloud (always)
  if [type] == "threshold_alert" {
    elasticsearch {
      hosts => ${ELASTIC_HOSTS}
      api_key => "${ELASTIC_API_KEY}"
      ssl_enabled => true
      index => "logstash-alerts-%{+YYYY.MM.dd}"
    }
  }
  
  # Send reset alerts to Elastic Cloud (always)
  else if [type] == "reset_alert" {
    elasticsearch {
      hosts => ${ELASTIC_HOSTS}
      api_key => "${ELASTIC_API_KEY}"
      ssl_enabled => true
      index => "logstash-alerts-%{+YYYY.MM.dd}"
    }
  }
  
  # Normal processing based on usage
  else {
    # Under 100% - send everything to Elastic Cloud
    if [@metadata][usage_percent] < 100 {
      elasticsearch {
        hosts => ${ELASTIC_HOSTS}
        api_key => "${ELASTIC_API_KEY}"
        ssl_enabled => true
        data_stream => true
      }
    }
    
    # 100-125% - drop metrics, send logs to Elastic Cloud
    else if [@metadata][usage_percent] >= 100 and [@metadata][usage_percent] < 125 {
      if [event][dataset] !~ /.*metrics.*/ and [metricset][name] !~ /.*/ {
        elasticsearch {
          hosts => ${ELASTIC_HOSTS}
          api_key => "${ELASTIC_API_KEY}"
          ssl_enabled => true
          data_stream => true
        }
      }
      # Metrics are implicitly dropped (no output)
    }
    
    # Over 125% - store locally for replay
    else {
      file {
        path => "${LOGSTASH_OVERFLOW_DIR:/opt/logstash/overflow}/replay-%{+YYYY-MM-dd}-%{+HH}.jsonl"
        codec => json_lines
        file_mode => 0644
        dir_mode => 0755
      }
    }
  }
}

# Separate pipeline for replaying overflow files (logstash-replay.conf)
# input {
#   file {
#     path => "/opt/logstash/overflow/replay-*.jsonl"
#     start_position => "beginning"
#     sincedb_path => "/opt/logstash/data/replay.sincedb"
#     codec => json_lines
#     tags => ["replay"]
#   }
# }
# 
# output {
#   if "replay" in [tags] {
#     elasticsearch {
#       hosts => ${ELASTIC_HOSTS}
#       api_key => "${ELASTIC_API_KEY}"
#       ssl_enabled => true
#       data_stream => true
#     }
#   }
# }